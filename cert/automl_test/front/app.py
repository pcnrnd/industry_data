# ëŒ€ì‹œë³´ë“œ UI
# https://docs.streamlit.io/knowledge-base/deploy/increase-file-uploader-limit-streamlit-cloud
# https://docs.streamlit.io/library/advanced-features/configuration#set-configuration-options
# ì‹¤í–‰ë°©ë²•1: streamlit run recomendation.py
# ì‹¤í–‰ë°©ë²•2: streamlit run recomendation.py --server.maxUploadSize 500 --server.maxMessageSize 500 (ì—…ë¡œë“œ íŒŒì¼ ìš©ëŸ‰ ì¦ëŒ€í•  ê²½ìš°)
# import time # ì½”ë“œ ì‹¤í–‰ ì‹œê°„ ì¸¡ì • ì‹œ ì‚¬ìš©
# sqlite:///./database/database.db
# AxiosError: Request failed with status code 403 in streamlit ë°œìƒ ì‹œ, enableXsrfProtection ì…ë ¥í•˜ì—¬ ì‹¤í–‰
# streamlit run recomendation.py --server.enableXsrfProtection false

# streamlit 1.24.0 ì´ìƒ ë²„ì „ì—ì„œ íŒŒì¼ ì—…ë¡œë“œí•  ê²½ìš° AxiosError: Request failed with status code 403 ë°œìƒí•  ìˆ˜ ìˆìŒ
# AxiosError 403 ì—ëŸ¬ ë°œìƒ ì‹œ streamlit==1.24.0 ë²„ì „ìœ¼ë¡œ ë³€ê²½ 
# pip install streamlit==1.24.0

# import ray
import json
import requests
import pandas as pd
import streamlit as st
from lib.template import Template
from lib.prepro import Preprocessing
# from database.connector import Database # , SelectTB
from io import StringIO
import time
import numpy as np
import plotly.express as px
from lib.research_prepro_engine import ResearchBasedRecommendationEngine
from lib.visualization_engine import VisualizationRecommendationEngine

def read_data_file(uploaded_file):
    """CSVì™€ Excel íŒŒì¼ì„ ëª¨ë‘ ì§€ì›í•˜ëŠ” íŒŒì¼ ì½ê¸° í•¨ìˆ˜"""
    file_name = uploaded_file.name.lower()
    
    try:
        if file_name.endswith(('.xlsx', '.xls')):
            # Excel íŒŒì¼ ì½ê¸° - ì»¬ëŸ¼ëª… ë³´ì¡´ ì˜µì…˜ ì¶”ê°€
            df = pd.read_excel(
                uploaded_file,
                engine='openpyxl',  # ëª…ì‹œì ìœ¼ë¡œ ì—”ì§„ ì§€ì •
                header=0,           # ì²« ë²ˆì§¸ í–‰ì„ í—¤ë”ë¡œ ì‚¬ìš©
                keep_default_na=True,  # ê¸°ë³¸ NA ê°’ ì²˜ë¦¬
                na_values=['', 'NULL', 'null', 'N/A', 'n/a']  # ì¶”ê°€ NA ê°’ë“¤
            )
            
            # ì»¬ëŸ¼ëª… ì •ë¦¬ (ê³µë°± ì œê±°, íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬)
            df.columns = df.columns.astype(str).str.strip()
            
            # st.success(f"Excel íŒŒì¼ '{uploaded_file.name}'ì„ ì„±ê³µì ìœ¼ë¡œ ì½ì—ˆìŠµë‹ˆë‹¤.")
            st.info(f"ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}, í–‰ ìˆ˜: {len(df)}")
            
            return df
            
        elif file_name.endswith('.csv'):
            # CSV íŒŒì¼ ì½ê¸° - ì—¬ëŸ¬ ì¸ì½”ë”© ì‹œë„
            encodings = ['utf-8', 'cp949', 'euc-kr', 'iso-8859-1', 'latin-1']
            
            for encoding in encodings:
                try:
                    uploaded_file.seek(0)
                    df = pd.read_csv(
                        uploaded_file, 
                        encoding=encoding,
                        header=0,
                        keep_default_na=True
                    )
                    
                    # ì»¬ëŸ¼ëª… ì •ë¦¬
                    df.columns = df.columns.astype(str).str.strip()
                    
                    # st.success(f"CSV íŒŒì¼ '{uploaded_file.name}'ì„ {encoding} ì¸ì½”ë”©ìœ¼ë¡œ ì„±ê³µì ìœ¼ë¡œ ì½ì—ˆìŠµë‹ˆë‹¤.")
                    # st.info(f"ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}, í–‰ ìˆ˜: {len(df)}")
                    
                    return df
                except UnicodeDecodeError:
                    continue
                except Exception as e:
                    continue
            
            st.error(f"CSV íŒŒì¼ '{uploaded_file.name}'ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
            
        else:
            st.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {uploaded_file.name}")
            return None
            
    except Exception as e:
        st.error(f"íŒŒì¼ ì½ê¸° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        return None

# import matplotlib.pyplot as plt
# import seaborn as sns


st.set_page_config(layout="wide")
st.sidebar.title("Details")

# ë¶„ë¥˜, ì´ìƒ íƒì§€ ë“± ì¶”ì²œë°›ì„ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì„ íƒ
option = st.sidebar.selectbox(
    'ë¨¸ì‹ ëŸ¬ë‹ ìœ í˜• ì„ íƒ', ('ë¶„ë¥˜', 'êµ°ì§‘', 'íšŒê·€'))
connecton_option = st.sidebar.selectbox(
    'Select how to upload data', ('File_upload'))

uploaded_file = None
df = None

if connecton_option == 'File_upload':
    uploaded_file = st.sidebar.file_uploader(
        "ë°ì´í„° íŒŒì¼ ì—…ë¡œë“œ", 
        type=["csv", "xlsx", "xls"]
    )
    
    if uploaded_file:
        df = read_data_file(uploaded_file)

with st.spinner('Wait for it...'):
    updated_df = None
    # Uploaded data Dashboard
    if uploaded_file is not None or df is not None:
        template = Template(df)
        st.subheader('ë°ì´í„° ë¶„ì„')
        col_list = df.columns.tolist() # ë°ì´í„° ì „ì²˜ë¦¬ ì˜µì…˜ ì„¤ì • ë¦¬ìŠ¤íŠ¸
        target_feture = ""
        if option == 'ë¶„ë¥˜' or option == 'íšŒê·€':
            target_feture = st.sidebar.multiselect('Select Target Column', options=col_list)
        elif option == 'êµ°ì§‘':
            st.sidebar.info("êµ°ì§‘ ë¶„ì„ì€ ë¹„ì§€ë„ í•™ìŠµì´ë¯€ë¡œ íƒ€ê²Ÿ ì»¬ëŸ¼ì´ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        data_to_drop = st.sidebar.multiselect('Drop Cloumns', options=col_list)
        data_for_labelencoding = st.sidebar.multiselect('Choose LabelEncoding column name', options=col_list)
        
        tab_eda_df, tab_eda_info, tab_Label_counts = st.tabs(['Original data', 'Data information', 'Target Data Counts']) # tab_Label_counts Labels counts
        # tab_eda_df, tab_eda_info tab UI Template
        template.eda_df(tab_eda_df=tab_eda_df, tab_eda_info=tab_eda_info)
        label_to_drop = ""
        with tab_Label_counts: # Target Data ì •ë³´ ì¶œë ¥ ë° ì‹œê°í™”
            if target_feture:            
                label_to_drop = template.label_to_drop(target_feture) # ì œê±°í•  Target ë°ì´í„° ì„ íƒ
            else:
                template.sample_df()

  
        if data_for_labelencoding:
            prepro = Preprocessing()
            if updated_df is None:
                # st.write(type(df[data_for_labelencoding]))
                df = prepro.encoded_df(df, data_for_labelencoding[0])
                updated_df = df
            if updated_df is not None:
                updated_df = prepro.encoded_df(updated_df, data_for_labelencoding[0])

       
        if data_to_drop:
            # íƒ€ê²Ÿ ì»¬ëŸ¼ì´ ì œê±° ëŒ€ìƒì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            if target_feture and any(target in data_to_drop for target in target_feture):
                st.error(f"íƒ€ê²Ÿ ì»¬ëŸ¼ '{target_feture}'ì€ ì œê±°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íƒ€ê²Ÿ ì»¬ëŸ¼ì„ ë‹¤ì‹œ ì„ íƒí•˜ê±°ë‚˜ ì œê±° ëŒ€ìƒì„ ìˆ˜ì •í•´ì£¼ì„¸ìš”.")
                st.stop()
            
            # ì œê±°í•  ì»¬ëŸ¼ë“¤ì„ í•œ ë²ˆì— ì œê±°
            updated_df = df.drop(data_to_drop, axis=1)

     
        try:
            if label_to_drop:
                target_feture = target_feture[0]
                label_to_drop = label_to_drop[0]
                updated_df = df[df[target_feture] != label_to_drop]
        except ValueError:
            st.write('1ê°œ ì´ìƒ ë°ì´í„°ê°€ ë‚¨ì•„ìˆì–´ì•¼ í•©ë‹ˆë‹¤.')

        # ë°ì´í„° ì „ì²˜ë¦¬ëœ ë°ì´í„° ì¶œë ¥
        if updated_df is not None: 
            st.subheader('ë°ì´í„° ì „ì²˜ë¦¬')
            st.dataframe(updated_df, use_container_width=True)
        
        if st.sidebar.button("ì´ˆê¸°í™”", use_container_width=True):
            st.cache_resource.clear()


#################### Starting ML traning
        button_for_training = st.sidebar.button("ë¨¸ì‹ ëŸ¬ë‹ í…ŒìŠ¤íŠ¸ ì‹¤í–‰", key="button1", use_container_width=True) 
        if button_for_training: # ë¶„ë¥˜, ì´ìƒíƒì§€ ì˜µì…˜ì— ë”°ë¼ ë¨¸ì‹ ëŸ¬ë‹ í•™ìŠµ ì§„í–‰
            start_time = time.time()
            # start_time = time.time() # í•™ìŠµ ì‹œê°„ ì²´í¬ ì‹œ ì„¤ì •

            if option == 'ë¶„ë¥˜' or option == 'êµ°ì§‘' or option == 'íšŒê·€':
                st.subheader('ë¨¸ì‹ ëŸ¬ë‹ í•™ìŠµ ê²°ê³¼')
                with st.spinner('Wait for it...'):
                    if updated_df is None:
                        updated_df = df   

                    # ë°ì´í„° ê²€ì¦
                    if option != 'êµ°ì§‘':  # êµ°ì§‘ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ íƒ€ê²Ÿ ì»¬ëŸ¼ ê²€ì¦
                        if target_feture is None or len(target_feture) == 0:
                            st.error("íƒ€ê²Ÿ ì»¬ëŸ¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
                            st.stop()
                    
                    # ìˆ˜ì¹˜í˜• ë°ì´í„°ë§Œ ì„ íƒ
                    numeric_df = updated_df.select_dtypes(include=[np.number])
                    if len(numeric_df.columns) == 0:
                        st.error("ìˆ˜ì¹˜í˜• ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                        st.stop()
                    
                    # ê° ë¨¸ì‹ ëŸ¬ë‹ ìœ í˜•ë³„ ê²€ì¦
                    if option == 'ë¶„ë¥˜':
                        # ë¶„ë¥˜: íƒ€ê²Ÿì´ ë²”ì£¼í˜•ì´ì–´ì•¼ í•¨
                        if target_feture[0] not in updated_df.columns:
                            st.error(f"íƒ€ê²Ÿ ì»¬ëŸ¼ '{target_feture[0]}'ì´ ë°ì´í„°ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                            st.stop()
                        
                        # ë¶„ë¥˜ì˜ ê²½ìš° íƒ€ê²Ÿì´ ë²”ì£¼í˜•ì´ì–´ì•¼ í•˜ë¯€ë¡œ ìˆ˜ì¹˜í˜•ì´ ì•„ë‹Œ ì»¬ëŸ¼ë„ í—ˆìš©
                        target_unique_count = updated_df[target_feture[0]].nunique()
                        if target_unique_count < 2:
                            st.error("ë¶„ë¥˜ë¥¼ ìœ„í•´ì„œëŠ” íƒ€ê²Ÿ ì»¬ëŸ¼ì— ìµœì†Œ 2ê°œ ì´ìƒì˜ í´ë˜ìŠ¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
                            st.stop()
                        elif target_unique_count > 50:
                            st.warning(f"íƒ€ê²Ÿ ì»¬ëŸ¼ì— {target_unique_count}ê°œì˜ í´ë˜ìŠ¤ê°€ ìˆìŠµë‹ˆë‹¤. ë‹¤ì¤‘ ë¶„ë¥˜ê°€ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    
                    elif option == 'íšŒê·€':
                        # íšŒê·€: íƒ€ê²Ÿì´ ìˆ˜ì¹˜í˜•ì´ì–´ì•¼ í•¨
                        if target_feture[0] not in numeric_df.columns:
                            st.error(f"íšŒê·€ë¥¼ ìœ„í•´ì„œëŠ” íƒ€ê²Ÿ ì»¬ëŸ¼ '{target_feture[0]}'ì´ ìˆ˜ì¹˜í˜•ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
                            st.stop()
                        
                        # íšŒê·€ì˜ ê²½ìš° íƒ€ê²Ÿì—ì„œ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ì œì™¸
                        feature_cols = [col for col in numeric_df.columns if col != target_feture[0]]
                        if len(feature_cols) == 0:
                            st.error("íšŒê·€ë¥¼ ìœ„í•œ íŠ¹ì„± ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                            st.stop()
                        numeric_df = updated_df[feature_cols + [target_feture[0]]]
                    
                    elif option == 'êµ°ì§‘':
                        # êµ°ì§‘: íƒ€ê²Ÿ ì»¬ëŸ¼ì´ ì—†ì–´ì•¼ í•¨ (ë¹„ì§€ë„ í•™ìŠµ)
                        # st.warning("êµ°ì§‘ ë¶„ì„ì€ ë¹„ì§€ë„ í•™ìŠµì´ë¯€ë¡œ íƒ€ê²Ÿ ì»¬ëŸ¼ì„ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                        # êµ°ì§‘ì˜ ê²½ìš° ëª¨ë“  ìˆ˜ì¹˜í˜• ë°ì´í„°ë¥¼ íŠ¹ì„±ìœ¼ë¡œ ì‚¬ìš©
                        feature_cols = numeric_df.columns.tolist()
                        if len(feature_cols) == 0:
                            st.error("êµ°ì§‘ ë¶„ì„ì„ ìœ„í•œ íŠ¹ì„± ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                            st.stop()
                        numeric_df = updated_df[feature_cols]
                        # êµ°ì§‘ì˜ ê²½ìš° íƒ€ê²Ÿ ì •ë³´ë¥¼ ë¹ˆ ë¬¸ìì—´ë¡œ ì„¤ì • (ë°±ì—”ë“œì—ì„œ ë¬´ì‹œë¨)
                        target_feture = ""

                    
                    json_data = numeric_df.to_json() # pandas DataFrameë¥¼ json í˜•íƒœë¡œ ë³€í™˜
                    data_dump = json.dumps({'json_data':json_data, 'target': target_feture}) # í•™ìŠµ ë°ì´í„°, Target Data ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ì§ë ¬í™”(serialize)
                    data = json.loads(data_dump) # jsonì„ íŒŒì´ì¬ ê°ì²´ë¡œ ë³€í™˜
                    
                    try:
                        if option == 'ë¶„ë¥˜':
                            response = requests.post('http://127.0.0.1:8000/automl/classification', json=data, timeout=300)
                        elif option == 'êµ°ì§‘':
                            response = requests.post('http://127.0.0.1:8000/automl/clustering', json=data, timeout=300)
                        elif option == 'íšŒê·€':
                            response = requests.post('http://127.0.0.1:8000/automl/regression', json=data, timeout=300)
                        
                        if response.status_code == 200: 
                            json_data = response.json() 
                            data = json.loads(json_data['result'])
                             # ë¶„ë¥˜ ëª¨ë¸
                            if option == 'ë¶„ë¥˜':
                                # ëª¨ë“  í‰ê°€ì§€í‘œë¥¼ í†µí•©í•˜ì—¬ ë³´ì—¬ì£¼ê¸°
                                clf_results = {}
                                scoring_methods = ['accuracy', 'recall', 'precision', 'f1_weighted']
                                
                                # ê° ì§€í‘œë³„ ê²°ê³¼ ìˆ˜ì§‘
                                for i, scoring_method in enumerate(scoring_methods):
                                    if str(i) in data:
                                        best_json = data[str(i)]["best"]
                                        best_data = json.loads(best_json)
                                        clf_results[scoring_method] = best_data.get('models', {})
                                
                                # ëª¨ë“  ëª¨ë¸ëª… ìˆ˜ì§‘
                                all_models = set()
                                for models_dict in clf_results.values():
                                    all_models.update(models_dict.keys())
                                
                                # í†µí•© DataFrame ìƒì„±
                                integrated_data = []
                                for model_name in all_models:
                                    model_scores = {'Model': model_name}
                                    for scoring_method in scoring_methods:
                                        if scoring_method in clf_results:
                                            model_scores[scoring_method] = clf_results[scoring_method].get(model_name, 0.0)
                                        else:
                                            model_scores[scoring_method] = 0.0
                                    integrated_data.append(model_scores)
                                
                                # DataFrame ìƒì„± ë° ì •ë ¬
                                if integrated_data:
                                    sorted_df = pd.DataFrame(integrated_data).set_index('Model')
                                    # f1_weighted ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ì—†ìœ¼ë©´ accuracy)
                                    if 'f1_weighted' in sorted_df.columns:
                                        sorted_df = sorted_df.sort_values(by='f1_weighted', ascending=False)
                                    elif 'accuracy' in sorted_df.columns:
                                        sorted_df = sorted_df.sort_values(by='accuracy', ascending=False)
                                    else:
                                        sorted_df = sorted_df.sort_values(by=sorted_df.columns[0], ascending=False)
                                else:
                                    sorted_df = pd.DataFrame({'Score': [0.0]}, index=['No Models'])
                            if option == 'íšŒê·€':
                                # ëª¨ë“  í‰ê°€ì§€í‘œë¥¼ í†µí•©í•˜ì—¬ ë³´ì—¬ì£¼ê¸°
                                reg_results = {}
                                scoring_methods = ['neg_mean_squared_error', 'neg_mean_absolute_error']
                                
                                # ê° ì§€í‘œë³„ ê²°ê³¼ ìˆ˜ì§‘
                                for i, scoring_method in enumerate(scoring_methods):
                                    if str(i) in data:
                                        best_json = data[str(i)]["best"]
                                        best_data = json.loads(best_json)
                                        reg_results[scoring_method] = best_data.get('models', {})
                                
                                # ëª¨ë“  ëª¨ë¸ëª… ìˆ˜ì§‘
                                all_models = set()
                                for models_dict in reg_results.values():
                                    all_models.update(models_dict.keys())
                                
                                # í†µí•© DataFrame ìƒì„±
                                integrated_data = []
                                for model_name in all_models:
                                    model_scores = {'Model': model_name}
                                    for scoring_method in scoring_methods:
                                        if scoring_method in reg_results:
                                            model_scores[scoring_method] = reg_results[scoring_method].get(model_name, 0.0)
                                        else:
                                            model_scores[scoring_method] = 0.0
                                    integrated_data.append(model_scores)
                                
                                # DataFrame ìƒì„± ë° ì •ë ¬
                                if integrated_data:
                                    sorted_df = pd.DataFrame(integrated_data).set_index('Model')
                                    # neg_mean_squared_error ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ìŒìˆ˜ê°’ì´ë¯€ë¡œ ë‚´ë¦¼ì°¨ìˆœ)
                                    if 'neg_mean_squared_error' in sorted_df.columns:
                                        sorted_df = sorted_df.sort_values(by='neg_mean_squared_error', ascending=False)
                                    else:
                                        sorted_df = sorted_df.sort_values(by=sorted_df.columns[0], ascending=False)
                                else:
                                    sorted_df = pd.DataFrame({'Score': [0.0]}, index=['No Models'])
                            if option == 'êµ°ì§‘':
                                # ëª¨ë“  í‰ê°€ì§€í‘œë¥¼ í†µí•©í•˜ì—¬ ë³´ì—¬ì£¼ê¸°
                                cluster_results = {}
                                scoring_methods = ['silhouette', 'calinski_harabasz', 'davies_bouldin']
                                
                                # ê° ì§€í‘œë³„ ê²°ê³¼ ìˆ˜ì§‘
                                for i, scoring_method in enumerate(scoring_methods):
                                    if str(i) in data:
                                        best_json = data[str(i)]["best"]
                                        best_data = json.loads(best_json)
                                        cluster_results[scoring_method] = best_data.get('models', {})
                                
                                # ëª¨ë“  ëª¨ë¸ëª… ìˆ˜ì§‘
                                all_models = set()
                                for models_dict in cluster_results.values():
                                    all_models.update(models_dict.keys())
                                
                                # í†µí•© DataFrame ìƒì„±
                                integrated_data = []
                                for model_name in all_models:
                                    model_scores = {'Model': model_name}
                                    for scoring_method in scoring_methods:
                                        if scoring_method in cluster_results:
                                            model_scores[scoring_method] = cluster_results[scoring_method].get(model_name, 0.0)
                                        else:
                                            model_scores[scoring_method] = 0.0
                                    integrated_data.append(model_scores)
                                
                                # DataFrame ìƒì„± ë° ì •ë ¬
                                if integrated_data:
                                    sorted_df = pd.DataFrame(integrated_data).set_index('Model')
                                    # silhouette ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
                                    if 'silhouette' in sorted_df.columns:
                                        sorted_df = sorted_df.sort_values(by='silhouette', ascending=False)
                                    elif 'calinski_harabasz' in sorted_df.columns:
                                        sorted_df = sorted_df.sort_values(by='calinski_harabasz', ascending=False)
                                    elif 'davies_bouldin' in sorted_df.columns:
                                        sorted_df = sorted_df.sort_values(by='davies_bouldin', ascending=True)  # ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
                                    else:
                                        sorted_df = sorted_df.sort_values(by=sorted_df.columns[0], ascending=False)
                                else:
                                    sorted_df = pd.DataFrame({'Score': [0.0]}, index=['No Models'])
                                
                                # êµ°ì§‘ ë¶„ì„ ê²°ê³¼ ì •ë ¬ ë¡œì§ ì œê±° (ì´ë¯¸ ì •ë ¬ë¨)
                            # sorted_concat_df = concat_df.sort_values(by='f1_weighted', ascending=False)

                            # ëª¨ë¸ ì¶”ì²œ
                            # ëª¨ë“  í‰ê°€ì§€í‘œë¥¼ í¬í•¨í•œ ìš°ìƒí–¥ ë¼ì¸ ì°¨íŠ¸
                            chart_df = sorted_df.reset_index()
                            # ìš°ìƒí–¥ ì‹œê°í™”ë¥¼ ìœ„í•´ ìˆœì„œë¥¼ ë’¤ì§‘ê¸° (ë‚®ì€ ì„±ëŠ¥ë¶€í„° ë†’ì€ ì„±ëŠ¥ ìˆœì„œ)
                            chart_df = chart_df.iloc[::-1].reset_index(drop=True)
                            
                            # ëª¨ë“  í‰ê°€ì§€í‘œë¥¼ long formatìœ¼ë¡œ ë³€í™˜
                            chart_df_long = chart_df.melt(
                                id_vars=['Model'], 
                                var_name='Metric', 
                                value_name='Score'
                            )
                            
                            fig = px.line(
                                chart_df_long,
                                x='Model',
                                y='Score',
                                color='Metric',
                                # title='ëª¨ë“  í‰ê°€ì§€í‘œ ì„±ëŠ¥ ë¹„êµ - ìš°ìƒí–¥ ì„±ëŠ¥ íŠ¸ë Œë“œ',
                                labels={'Model': 'ëª¨ë¸', 'Score': 'ì„±ëŠ¥ ì ìˆ˜', 'Metric': 'í‰ê°€ì§€í‘œ'},
                                markers=True
                            )
                            
                            # xì¶• ìˆœì„œ ê³ ì • (ë‚®ì€ ì„±ëŠ¥ë¶€í„° ë†’ì€ ì„±ëŠ¥ìœ¼ë¡œ ìš°ìƒí–¥)
                            fig.update_xaxes(categoryorder='array', categoryarray=chart_df['Model'].tolist())
                            fig.update_layout(
                                height=500,
                                xaxis_title="ëª¨ë¸",
                                yaxis_title="ì„±ëŠ¥ ì ìˆ˜",
                                legend_title="í‰ê°€ì§€í‘œ"
                            )
                            
                            # ë¼ì¸ ë° ë§ˆì»¤ ìŠ¤íƒ€ì¼ ì„¤ì •
                            fig.update_traces(
                                line=dict(width=3),
                                marker=dict(size=8)
                            )
                            
                            st.plotly_chart(fig, use_container_width=True)
                            
                            # í‰ê°€ì§€í‘œ ì„¤ëª… ì¶”ê°€
                            # if option == 'ë¶„ë¥˜':
                            #     st.subheader('ğŸ“Š ë¶„ë¥˜ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€')
                            #     st.info("""
                            #     **í‰ê°€ì§€í‘œ ì„¤ëª…:**
                            #     - **accuracy**: ì •í™•ë„ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
                            #     - **recall**: ì¬í˜„ìœ¨ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
                            #     - **precision**: ì •ë°€ë„ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)  
                            #     - **f1_weighted**: F1 ìŠ¤ì½”ì–´ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
                            #     """)
                            # elif option == 'íšŒê·€':
                            #     st.subheader('ğŸ“Š íšŒê·€ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€')
                            #     st.info("""
                            #     **í‰ê°€ì§€í‘œ ì„¤ëª…:**
                            #     - **neg_mean_squared_error**: ìŒì˜ í‰ê·  ì œê³± ì˜¤ì°¨ (0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ)
                            #     - **neg_mean_absolute_error**: ìŒì˜ í‰ê·  ì ˆëŒ€ ì˜¤ì°¨ (0ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì¢‹ìŒ)
                            #     """)
                            # elif option == 'êµ°ì§‘':
                            #     st.subheader('ğŸ“Š êµ°ì§‘ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€')
                            #     st.info("""
                            #     **í‰ê°€ì§€í‘œ ì„¤ëª…:**
                            #     - **silhouette**: ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ, -1~1)
                            #     - **calinski_harabasz**: ì¹¼ë¦°ìŠ¤í‚¤-í•˜ë¼ë°”ì¦ˆ ì§€ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
                            #     - **davies_bouldin**: ë°ì´ë¹„ìŠ¤-ë³¼ë”˜ ì§€ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
                            #     """)
                            
                            st.subheader('ğŸ¯ ëª¨ë¸ ë¹„êµ')
                            
                            # ì†Œìˆ˜ì  ìë¦¬ìˆ˜ í˜•ì‹ ì§€ì •
                            if not sorted_df.empty and 'Score' not in sorted_df.columns:
                                # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì— ëŒ€í•´ ì†Œìˆ˜ì  4ìë¦¬ë¡œ í‘œì‹œ
                                formatted_df = sorted_df.round(4)
                                st.dataframe(formatted_df, use_container_width=True)
                                
                            else:
                                st.dataframe(sorted_df, use_container_width=True)

                            # ê°œì„ ëœ ì¶”ì²œ ì‹œìŠ¤í…œ
                            data = updated_df
                            
                            # ë°ì´í„° ë¶„ì„ ë° íŠ¹ì„± ì¶”ì¶œ
                            numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
                            categorical_cols = data.select_dtypes(include=['object']).columns.tolist()
                            
                            # ë°ì´í„° íŠ¹ì„± ë¶„ì„
                            data_characteristics = {}
                            for col in numeric_cols:
                                col_data = data[col].dropna()
                                if len(col_data) > 0:
                                    data_characteristics[col] = {
                                        'mean': col_data.mean(),
                                        'std': col_data.std(),
                                        'skewness': col_data.skew(),
                                        'unique_count': col_data.nunique(),
                                        'missing_ratio': data[col].isnull().sum() / len(data)
                                    }
                            
                            # ì „ì²˜ë¦¬ ì¶”ì²œ (ResearchBasedRecommendationEngine ê¸°ë°˜)
                            preprocessing_recommendations = []
                            engine = ResearchBasedRecommendationEngine()
                            rec_result = engine.run(data)

                            for col in data.columns:
                                recs = rec_result['preprocessing'].get(col, [])
                                if not recs:
                                    continue
                                # ì¶”ì²œì„ í…Œì´ë¸” í˜•íƒœì— ë§ê²Œ ë¶„ë¦¬
                                primary = recs[0]['action'] if len(recs) > 0 else '-'
                                secondary = recs[1]['action'] if len(recs) > 1 else '-'
                                additional = ', '.join([r['action'] for r in recs[2:]]) if len(recs) > 2 else 'None'
                                preprocessing_recommendations.append({
                                    'Column': col,
                                    'Primary Recommendation': primary,
                                    'Secondary Recommendation': secondary,
                                    'Additional Techniques': additional
                                })
                            
                            # ì‹œê°í™” ì¶”ì²œ (ê°œì„ ëœ ì—”ì§„ ì‚¬ìš©)
                            visualization_recommendations = []
                            vis_engine = VisualizationRecommendationEngine()
                            vis_result = vis_engine.run(data)
                            
                            # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ì‹œê°í™” ì¶”ì²œ
                            for col in numeric_cols:
                                if col in vis_result['visualization']:
                                    recs = vis_result['visualization'][col]
                                    if not recs:
                                        continue
                                    
                                    # ì¶”ì²œì„ í…Œì´ë¸” í˜•íƒœì— ë§ê²Œ ë¶„ë¦¬
                                    primary = recs[0]['chart'] if len(recs) > 0 else '-'
                                    secondary = recs[1]['chart'] if len(recs) > 1 else '-'
                                    additional = ', '.join([r['chart'] for r in recs[2:]]) if len(recs) > 2 else 'None'
                                    
                                    visualization_recommendations.append({
                                        'Column': col,
                                        'Primary Visualization': primary,
                                        'Secondary Visualization': secondary,
                                        'Additional Charts': additional
                                    })
                            
                            # ë²”ì£¼í˜• ì»¬ëŸ¼ ì‹œê°í™” ì¶”ì²œ
                            for col in categorical_cols:
                                if col in vis_result['visualization']:
                                    recs = vis_result['visualization'][col]
                                    if not recs:
                                        continue
                                    
                                    # ì¶”ì²œì„ í…Œì´ë¸” í˜•íƒœì— ë§ê²Œ ë¶„ë¦¬
                                    primary = recs[0]['chart'] if len(recs) > 0 else '-'
                                    secondary = recs[1]['chart'] if len(recs) > 1 else '-'
                                    additional = ', '.join([r['chart'] for r in recs[2:]]) if len(recs) > 2 else 'None'
                                    
                                    visualization_recommendations.append({
                                        'Column': col,
                                        'Primary Visualization': primary,
                                        'Secondary Visualization': secondary,
                                        'Additional Charts': additional
                                    })
                            
                            # ìƒê´€ê´€ê³„ ì‹œê°í™” ì¶”ì²œ
                            if '_correlation' in vis_result['visualization']:
                                corr_recs = vis_result['visualization']['_correlation']
                                if corr_recs:
                                    visualization_recommendations.append({
                                        'Column': 'All Numeric Columns',
                                        'Primary Visualization': corr_recs[0]['chart'],
                                        'Secondary Visualization': corr_recs[1]['chart'] if len(corr_recs) > 1 else 'Correlation Heatmap',
                                        'Additional Charts': ', '.join([r['chart'] for r in corr_recs[2:]]) if len(corr_recs) > 2 else 'None'
                                    })
                            
                            # ì „ì²´ ë¶„í¬ ì‹œê°í™” ì¶”ì²œ
                            if '_distribution' in vis_result['visualization']:
                                dist_recs = vis_result['visualization']['_distribution']
                                if dist_recs:
                                    visualization_recommendations.append({
                                        'Column': 'Dataset Overview',
                                        'Primary Visualization': dist_recs[0]['chart'],
                                        'Secondary Visualization': dist_recs[1]['chart'] if len(dist_recs) > 1 else 'Summary Statistics',
                                        'Additional Charts': ', '.join([r['chart'] for r in dist_recs[2:]]) if len(dist_recs) > 2 else 'None'
                                    })
                            
                            # ëª¨ë¸ ì„±ëŠ¥ ê¸°ë°˜ ì¶”ì²œ
                            best_model = sorted_df.iloc[0] if len(sorted_df) > 0 else None

                            # ê²°ê³¼ í‘œì‹œ
                            prepro_recommendations_df = pd.DataFrame(preprocessing_recommendations)
                            vis_recommendations_df = pd.DataFrame(visualization_recommendations)
                            
                            
                            st.subheader("ğŸ”§ ì „ì²˜ë¦¬ ì¶”ì²œ ê²°ê³¼")
                            st.dataframe(prepro_recommendations_df, use_container_width=True)
                            
                            st.subheader("ğŸ“ˆ ì‹œê°í™” ì¶”ì²œ ê²°ê³¼")
                            st.dataframe(vis_recommendations_df, use_container_width=True)
                            
                            # ì‹œê°í™” ì¶”ì²œ ìƒì„¸ ì„¤ëª…
                            # st.subheader("ğŸ“Š ì‹œê°í™” ì¶”ì²œ ìƒì„¸ ì„¤ëª…")
                            # st.info("""
                            # **ì‹œê°í™” ìœ í˜•ë³„ ì„¤ëª…:**
                            
                            # **ìˆ˜ì¹˜í˜• ë°ì´í„°:**
                            # - **Histogram**: ê¸°ë³¸ ë¶„í¬ ë¶„ì„
                            # - **Box Plot**: ì´ìƒì¹˜ ë° ë¶„ìœ„ìˆ˜ ë¶„ì„
                            # - **Density Plot**: í™•ë¥  ë°€ë„ í•¨ìˆ˜
                            # - **Q-Q Plot**: ì •ê·œë¶„í¬ ê²€ì¦
                            # - **Violin Plot**: ë¶„í¬ í˜•íƒœì™€ ë°€ë„ ë™ì‹œ ë¶„ì„
                            
                            # **ë²”ì£¼í˜• ë°ì´í„°:**
                            # - **Bar Chart**: ê¸°ë³¸ ë¹ˆë„ ë¶„ì„
                            # - **Pie Chart**: ë¹„ìœ¨ ë¶„ì„ (ë‚®ì€ ì¹´ë””ë„ë¦¬í‹°)
                            # - **Horizontal Bar Chart**: ê¸´ ì¹´í…Œê³ ë¦¬ëª… ì²˜ë¦¬
                            # - **Word Cloud**: í…ìŠ¤íŠ¸ ë°ì´í„° ë¶„ì„
                            
                            # **ìƒê´€ê´€ê³„ ë¶„ì„:**
                            # - **Correlation Heatmap**: ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„
                            # - **Pair Plot**: ë‹¤ë³€ëŸ‰ ê´€ê³„ ë¶„ì„
                            
                            # **ì‹œê³„ì—´ ë°ì´í„°:**
                            # - **Time Series Plot**: ì‹œê°„ì— ë”°ë¥¸ ë³€í™”
                            # - **Seasonal Decomposition**: ê³„ì ˆì„± ë¶„ì„
                            # """)
                            
                            # í†µí•© ì¶”ì²œ ê²°ê³¼
                            if best_model is not None:
                                st.subheader("ğŸ¯ í†µí•© ì¶”ì²œ ê²°ê³¼")
                                best_model_name = sorted_df.index[0] # if len(sorted_df) > 0 else "Unknown"
                                
                                # ìµœì  ì „ì²˜ë¦¬ ë°©ë²• ì°¾ê¸°
                                best_preprocessing = 'N/A'
                                if len(prepro_recommendations_df) > 0:
                                    # ê°€ì¥ ì¤‘ìš”í•œ ì „ì²˜ë¦¬ ë°©ë²• ì„ íƒ (ì²« ë²ˆì§¸ ì¶”ì²œ)
                                    best_preprocessing = prepro_recommendations_df.iloc[0]['Primary Recommendation']
                                
                                # ìµœì  ì‹œê°í™” ë°©ë²• ì°¾ê¸°
                                best_visualization = 'N/A'
                                if len(vis_recommendations_df) > 0:
                                    # ì²« ë²ˆì§¸ ì‹œê°í™” ë°©ë²• ì„ íƒ
                                    best_visualization = vis_recommendations_df.iloc[0]['Primary Visualization']
                                
                                recommendation_summary = {
                                    'Best Model': best_model_name,
                                    'Key Preprocessing': best_preprocessing,
                                    'Key Visualization': best_visualization,
                                }
                                
                                summary_df = pd.DataFrame([recommendation_summary])
                                st.dataframe(summary_df, use_container_width=True)
                                
                                # ì¶”ì²œ ê·¼ê±° ì„¤ëª…
                                # st.subheader("ğŸ“‹ ì¶”ì²œ ê·¼ê±°")
                                # st.info(f"""
                                # **ëª¨ë¸ ì„ íƒ ê·¼ê±°:**
                                # - ì„ íƒëœ ëª¨ë¸ '{best_model_name}'ì€ ëª¨ë“  í‰ê°€ì§€í‘œì—ì„œ ìµœê³  ì„±ëŠ¥ì„ ë³´ì˜€ìŠµë‹ˆë‹¤.
                                
                                # **ì „ì²˜ë¦¬ ì¶”ì²œ ê·¼ê±°:**
                                # - ë°ì´í„° íŠ¹ì„± ë¶„ì„ì„ í†µí•´ ê°€ì¥ ì í•©í•œ ì „ì²˜ë¦¬ ë°©ë²•ì„ ì¶”ì²œí–ˆìŠµë‹ˆë‹¤.
                                
                                # **ì‹œê°í™” ì¶”ì²œ ê·¼ê±°:**
                                # - ë°ì´í„° íƒ€ì…ê³¼ ë¶„í¬ íŠ¹ì„±ì„ ê³ ë ¤í•˜ì—¬ ìµœì ì˜ ì‹œê°í™” ë°©ë²•ì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤.
                                
                                # **ë°ì´í„° íŠ¹ì„±:**
                                # - ì´ {len(data)}ê°œì˜ ìƒ˜í”Œê³¼ {len(data.columns)}ê°œì˜ íŠ¹ì„±ìœ¼ë¡œ êµ¬ì„±ëœ ë°ì´í„°ì…‹ì…ë‹ˆë‹¤.
                                # - ìˆ˜ì¹˜í˜• íŠ¹ì„±: {len(numeric_cols)}ê°œ, ë²”ì£¼í˜• íŠ¹ì„±: {len(categorical_cols)}ê°œ
                                # """)
                        else:
                            error_data = response.json()
                            st.error(f"API ì˜¤ë¥˜: {error_data.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
                            
                    except requests.exceptions.Timeout:
                        st.error("ìš”ì²­ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ì„œë²„ê°€ ì‘ë‹µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                    except requests.exceptions.ConnectionError:
                        st.error("ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë°±ì—”ë“œ ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
                    except Exception as e:
                        st.error(f"ìš”ì²­ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")