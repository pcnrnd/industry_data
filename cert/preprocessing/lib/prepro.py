import polars as pl
from PIL import Image
import psutil
import time
import os
import subprocess
import logging
from pathlib import Path
from typing import List, Tuple, Optional, Dict, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
import pandas as pd
import signal
import functools
import uuid


def timeout_handler(signum, frame):
    """íƒ€ì„ì•„ì›ƒ í•¸ë“¤ëŸ¬"""
    raise TimeoutError("íŒŒì¼ ì²˜ë¦¬ ì‹œê°„ ì´ˆê³¼")


def timeout(seconds):
    """íƒ€ì„ì•„ì›ƒ ë°ì½”ë ˆì´í„°"""
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            # Windowsì—ì„œëŠ” signal.SIGALRMì´ ì§€ì›ë˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë‹¤ë¥¸ ë°©ì‹ ì‚¬ìš©
            if os.name == 'nt':  # Windows
                import threading
                result = [None]
                exception = [None]
                
                def target():
                    try:
                        result[0] = func(*args, **kwargs)
                    except Exception as e:
                        exception[0] = e
                
                thread = threading.Thread(target=target)
                thread.daemon = True
                thread.start()
                thread.join(seconds)
                
                if thread.is_alive():
                    # ìŠ¤ë ˆë“œê°€ ì—¬ì „íˆ ì‹¤í–‰ ì¤‘ì´ë©´ íƒ€ì„ì•„ì›ƒ
                    raise TimeoutError(f"í•¨ìˆ˜ ì‹¤í–‰ ì‹œê°„ ì´ˆê³¼ ({seconds}ì´ˆ)")
                
                if exception[0]:
                    raise exception[0]
                
                return result[0]
            else:  # Unix/Linux
                old_handler = signal.signal(signal.SIGALRM, timeout_handler)
                signal.alarm(seconds)
                try:
                    result = func(*args, **kwargs)
                finally:
                    signal.alarm(0)
                    signal.signal(signal.SIGALRM, old_handler)
                return result
        return wrapper
    return decorator


class UUIDProcessor:
    """UUID ë° íŒŒì¼ ë©”íƒ€ë°ì´í„° ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    @staticmethod
    def generate_uuid() -> str:
        """UUID ìƒì„±"""
        return str(uuid.uuid4())
    
    @staticmethod
    def convert_file_size(size_bytes: int) -> str:
        """íŒŒì¼ í¬ê¸°ë¥¼ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ë³€í™˜"""
        if size_bytes < 1024:
            return f"{size_bytes} B"
        elif size_bytes < 1024 * 1024:
            return f"{size_bytes / 1024:.1f} KB"
        elif size_bytes < 1024 * 1024 * 1024:
            return f"{size_bytes / (1024 * 1024):.1f} MB"
        else:
            return f"{size_bytes / (1024 * 1024 * 1024):.1f} GB"
    
    @staticmethod
    def extract_data_category(full_path: str) -> str:
        """ê²½ë¡œì—ì„œ ë°ì´í„° ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ"""
        if 'cctv_data' in full_path:
            return 'cctv_data'
        elif 'sand_data' in full_path:
            return 'sand_data'
        else:
            return 'other'


class DataProcessor:
    """ë°ì´í„° ì²˜ë¦¬ì™€ ê´€ë ¨ëœ ê¸°ëŠ¥ì„ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤"""
    
    @staticmethod
    def format_bytes(size: int) -> str:
        """byteë¥¼ KB, MB, GB, TB ë“±ìœ¼ë¡œ ë³€ê²½í•˜ëŠ” í•¨ìˆ˜"""
        volum = 1024
        n = 0
        volum_labels = {0: 'B', 1: 'KB', 2: 'MB', 3: 'GB', 4: 'TB'}
        original_size = size
        while original_size >= volum and n < len(volum_labels) - 1:
            original_size /= volum
            n += 1
        return f"{original_size:.1f} {volum_labels[n]}"
    
    @staticmethod
    def get_directory_info(directory: str, summary_only: bool = False) -> Optional[str]:
        """ë””ë ‰í† ë¦¬ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ (ê°„ë‹¨í•œ ìš”ì•½ë§Œ)"""
        try:
            path = Path(directory)
            if not path.exists():
                return f"ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {directory}"
            
            if summary_only:
                # ë¹ ë¥¸ ìš”ì•½(í´ë”/íŒŒì¼ ê°œìˆ˜ë§Œ)
                file_count = 0
                dir_count = 0
                for root, dirs, files in os.walk(path):
                    dir_count += len(dirs)
                    file_count += len(files)
                return f"[ìš”ì•½] í´ë” {dir_count}ê°œ, íŒŒì¼ {file_count}ê°œ"
            else:
                # ì „ì²´ ë””ë ‰í† ë¦¬ êµ¬ì¡° (ê°œì„ ëœ í˜•íƒœ)
                result = f"ë””ë ‰í† ë¦¬ êµ¬ì¡°: {directory}\n"
                result += "=" * 80 + "\n"
                
                # ìµœëŒ€ ê¹Šì´ ì œí•œ (ë„ˆë¬´ ê¹Šì€ êµ¬ì¡° ë°©ì§€)
                max_depth = 5
                max_files_per_dir = 10
                
                for root, dirs, files in os.walk(path):
                    # ê¹Šì´ ê³„ì‚°
                    depth = root.replace(str(path), '').count(os.sep)
                    if depth > max_depth:
                        continue
                    
                    # ë“¤ì—¬ì“°ê¸°
                    indent = '  ' * depth
                    dir_name = os.path.basename(root) if root != str(path) else os.path.basename(path)
                    
                    # ë””ë ‰í† ë¦¬ í‘œì‹œ
                    if depth == 0:
                        result += f"{indent}ğŸ“ {dir_name}/\n"
                    else:
                        result += f"{indent}ğŸ“ {dir_name}/\n"
                    
                    # íŒŒì¼ í‘œì‹œ (ì²˜ìŒ 10ê°œë§Œ)
                    if files:
                        for i, file in enumerate(files[:max_files_per_dir]):
                            file_size = ""
                            try:
                                file_path = os.path.join(root, file)
                                if os.path.exists(file_path):
                                    size = os.path.getsize(file_path)
                                    if size > 1024 * 1024:  # 1MB ì´ìƒ
                                        file_size = f" ({DataProcessor.format_bytes(size)})"
                            except:
                                pass
                            result += f"{indent}  ğŸ“„ {file}{file_size}\n"
                        
                        if len(files) > max_files_per_dir:
                            result += f"{indent}  ... ({len(files) - max_files_per_dir}ê°œ ë”)\n"
                    
                    # í•˜ìœ„ ë””ë ‰í† ë¦¬ ìˆ˜ í‘œì‹œ
                    if dirs:
                        result += f"{indent}  ğŸ“‚ í•˜ìœ„ ë””ë ‰í† ë¦¬: {len(dirs)}ê°œ\n"
                
                result += "=" * 80 + "\n"
                return result
        except Exception as e:
            logging.error(f"ë””ë ‰í† ë¦¬ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return None
    
    @staticmethod
    def split_file_list(file_paths: List[Path], batch_size: int) -> List[List[Path]]:
        """íŒŒì¼ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ë¶„í• """
        batches = []
        for i in range(0, len(file_paths), batch_size):
            batch = file_paths[i:i + batch_size]
            batches.append(batch)
        return batches
    
    @staticmethod
    def process_batch(batch: List[Path], batch_num: int, total_batches: int, 
                     max_workers: int = 4) -> Tuple[pd.DataFrame, str]:
        """ë‹¨ì¼ ë°°ì¹˜ ì²˜ë¦¬ (ë¡œê¹… ìµœì í™”)"""
        batch_start_time = time.time()
        
        # ë°°ì¹˜ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ (ë°°ì¹˜ ì •ë³´ ì „ë‹¬)
        metadata_list = DataProcessor.extract_file_metadata(
            batch, max_workers, batch_num, total_batches
        )
        
        # DataFrame ìƒì„±
        df = pl.DataFrame(metadata_list)
        pandas_df = df.to_pandas()
        
        # ë°°ì¹˜ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
        batch_end_time = time.time()
        batch_elapsed = batch_end_time - batch_start_time
        
        # ë°°ì¹˜ ì •ë³´ ìƒì„±
        batch_size_bytes = sum(pandas_df['file_size'])
        batch_info = (f"ë°°ì¹˜ {batch_num}/{total_batches}: {len(batch)}ê°œ íŒŒì¼, "
                     f"{DataProcessor.format_bytes(batch_size_bytes)}, "
                     f"ì²˜ë¦¬ì‹œê°„: {batch_elapsed:.1f}ì´ˆ")
        
        return pandas_df, batch_info
    
    @staticmethod
    @timeout(30)  # 30ì´ˆ íƒ€ì„ì•„ì›ƒ
    def _extract_single_image_resolution(file_path: Path) -> Tuple[int, int]:
        """ë‹¨ì¼ ì´ë¯¸ì§€ í•´ìƒë„ ì¶”ì¶œ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
        try:
            with Image.open(file_path) as img:
                return img.size
        except Exception as e:
            logging.warning(f"ì´ë¯¸ì§€ í•´ìƒë„ ì¶”ì¶œ ì‹¤íŒ¨ {file_path}: {e}")
            return (0, 0)
    
    @staticmethod
    def filter_non_zip_files(file_paths: List[Path]) -> List[Path]:
        """ZIP íŒŒì¼ì„ ì œì™¸í•˜ê³  ì‹¤ì œ ë°ì´í„° íŒŒì¼ë§Œ í•„í„°ë§"""
        filtered_paths = []
        for path in file_paths:
            if path.suffix.lower() != '.zip':
                filtered_paths.append(path)
        return filtered_paths
    
    @staticmethod
    def extract_file_metadata(file_paths: List[Path], max_workers: int = 4, 
                             batch_num: int = 0, total_batches: int = 0) -> List[Dict[str, Any]]:
        """íŒŒì¼ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ - ê¸°ë³¸ ì •ë³´ + UUID + íŒŒìƒ ì»¬ëŸ¼ (ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”)"""
        metadata_list = []
        total_files = len(file_paths)
        
        # ë°°ì¹˜ ë‚´ë¶€ì—ì„œëŠ” ë¡œê¹…í•˜ì§€ ì•ŠìŒ (ì „ì²´ ì§„í–‰ë¥ ë§Œ í‘œì‹œ)
        for i, path in enumerate(file_paths):
            try:
                # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                metadata = {
                    'full_path': str(path),
                    'file_id': UUIDProcessor.generate_uuid(),  # file_id ëŒ€ì‹  uuid ì‚¬ìš©
                    'folder_name': path.parent.name,
                    'file_size': path.stat().st_size if path.exists() else 0,
                    'file_size_readable': UUIDProcessor.convert_file_size(
                        path.stat().st_size if path.exists() else 0
                    ),
                    'file_type': path.suffix.lower(),
                    'data_category': UUIDProcessor.extract_data_category(str(path))
                }
                
                metadata_list.append(metadata)
            except Exception as e:
                logging.warning(f"íŒŒì¼ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨ {path}: {e}")
                # ì‹¤íŒ¨í•œ ê²½ìš°ì—ë„ ê¸°ë³¸ ì •ë³´ëŠ” í¬í•¨
                metadata = {
                    'full_path': str(path),
                    'uuid': UUIDProcessor.generate_uuid(),
                    'folder_name': path.parent.name,
                    'file_size': 0,
                    'file_size_readable': '0 B',
                    'file_type': path.suffix.lower(),
                    'data_category': 'unknown'
                }
                metadata_list.append(metadata)
        
        return metadata_list
    
    @staticmethod
    def extract_image_resolutions(file_paths: List[Path], max_workers: int = 4,
                                 batch_num: int = 0, total_batches: int = 0) -> List[Tuple[int, int]]:
        """ì´ë¯¸ì§€ í•´ìƒë„ ì¶”ì¶œ (width, height) - ë³‘ë ¬ ì²˜ë¦¬ ì ìš© (ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”)"""
        resolutions = []
        total_files = len(file_paths)
        
        # íŒŒì¼ ìˆ˜ê°€ ì ìœ¼ë©´ ìˆœì°¨ ì²˜ë¦¬
        if len(file_paths) < 10:
            for i, path in enumerate(file_paths):
                resolutions.append(DataProcessor._extract_single_image_resolution(path))
        else:
            # ë³‘ë ¬ ì²˜ë¦¬
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_path = {executor.submit(DataProcessor._extract_single_image_resolution, path): path 
                                for path in file_paths}
                
                for i, future in enumerate(as_completed(future_to_path)):
                    resolutions.append(future.result())
        
        return resolutions
    
    @staticmethod
    def make_polars_dataframe(file_paths: List[Path], max_workers: int = 4) -> pl.DataFrame:
        """polars DataFrame ìƒì„± - ê¸°ë³¸ ì •ë³´ + ì´ë¯¸ì§€ í•´ìƒë„"""
        logging.info(f"ì´ {len(file_paths)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘")
        
        # ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        metadata_list = DataProcessor.extract_file_metadata(file_paths, max_workers)
        
        # ë©”íƒ€ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        df = pl.DataFrame(metadata_list)
        
        # ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬
        df = df.select(['full_path', 'uuid', 'folder_name', 'file_size', 
                       'file_size_readable', 'file_type', 'data_category'])
        
        logging.info(f"DataFrame ìƒì„± ì™„ë£Œ: {len(df)} í–‰, {len(df.columns)} ì—´")
        return df
    
    @staticmethod
    def make_pandas_dataframe(file_paths: List[Path], max_workers: int = 4) -> pd.DataFrame:
        """pandas DataFrame ìƒì„±"""
        df = DataProcessor.make_polars_dataframe(file_paths, max_workers)
        return df.to_pandas()
    
    @staticmethod
    def log_system_resources() -> str:
        """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ê³  ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜"""
        try:
            memory_info = psutil.virtual_memory()
            cpu_percent = psutil.cpu_percent(interval=1)
            
            # Windowsì—ì„œëŠ” ë£¨íŠ¸ ë””ìŠ¤í¬ ëŒ€ì‹  í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬ ì‚¬ìš©
            current_dir = Path.cwd()
            disk_usage = psutil.disk_usage(str(current_dir))
            
            # G ë“œë¼ì´ë¸Œ ì •ë³´ ì¶”ê°€
            g_drive_info = ""
            try:
                g_disk_usage = psutil.disk_usage('/mnt/external_drive/industry_data')
                g_drive_info = f"""
            ë“œë¼ì´ë¸Œ ì •ë³´:
            --------------------------------------
            ë“œë¼ì´ë¸Œ ì „ì²´ ìš©ëŸ‰: {DataProcessor.format_bytes(g_disk_usage.total)} 
            ë“œë¼ì´ë¸Œ ì‚¬ìš©ëœ ìš©ëŸ‰: {DataProcessor.format_bytes(g_disk_usage.used)} 
            ë“œë¼ì´ë¸Œ ì‚¬ìš© í¼ì„¼íŠ¸: {g_disk_usage.percent}%
            ë“œë¼ì´ë¸Œ ì—¬ìœ  ìš©ëŸ‰: {DataProcessor.format_bytes(g_disk_usage.free)}
            --------------------------------------"""
            except Exception as e:
                g_drive_info = f"""
            ë“œë¼ì´ë¸Œ ì •ë³´:
            --------------------------------------
            ë“œë¼ì´ë¸Œ ì ‘ê·¼ ë¶ˆê°€: {e}
            --------------------------------------"""
            
            current_time = time.strftime('%Y-%m-%d %H:%M:%S') # í˜„ì¬ ì‹œê°„ ê¸°ë¡
            # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì •ë³´ë¥¼ í‘œ í˜•íƒœë¡œ ìƒì„±
            system_info = f"""
            ì‚¬ìš© ì¤‘ì¸ ìì› í™•ì¸:
            --------------------------------------
            ì „ì²´ ë©”ëª¨ë¦¬: {DataProcessor.format_bytes(memory_info.total)} 
            ì‚¬ìš© ê°€ëŠ¥í•œ ë©”ëª¨ë¦¬: {DataProcessor.format_bytes(memory_info.available)} 
            ì‚¬ìš©ëœ ë©”ëª¨ë¦¬: {DataProcessor.format_bytes(memory_info.used)} 
            ë©”ëª¨ë¦¬ ì‚¬ìš© í¼ì„¼íŠ¸: {memory_info.percent}%
            CPU ì‚¬ìš© í¼ì„¼íŠ¸: {cpu_percent}%
            í˜„ì¬ ë””ìŠ¤í¬ ìš©ëŸ‰: {DataProcessor.format_bytes(disk_usage.total)} 
            í˜„ì¬ ì‚¬ìš©ëœ ë””ìŠ¤í¬ ìš©ëŸ‰: {DataProcessor.format_bytes(disk_usage.used)} 
            í˜„ì¬ ë””ìŠ¤í¬ ì‚¬ìš© í¼ì„¼íŠ¸: {disk_usage.percent}%{g_drive_info}
            """
            return system_info
        except Exception as e:
            logging.error(f"ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return "ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨"
    
    @staticmethod
    def setup_logging(log_file: str = './log/prepro.log', level: int = logging.INFO) -> None:
        """ë¡œê¹… ì„¤ì •ì„ ì´ˆê¸°í™”í•˜ëŠ” í•¨ìˆ˜"""
        try:
            os.makedirs(os.path.dirname(log_file), exist_ok=True)  # log ë””ë ‰í† ë¦¬ ìƒì„±
            
            # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°
            for handler in logging.getLogger().handlers[:]:
                logging.getLogger().removeHandler(handler)
            
            # íŒŒì¼ í•¸ë“¤ëŸ¬ ì„¤ì •
            file_handler = logging.FileHandler(log_file, encoding='utf-8')
            file_handler.setLevel(level)
            file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            file_handler.setFormatter(file_formatter)
            
            # ì½˜ì†” í•¸ë“¤ëŸ¬ ì„¤ì • (ì§„í–‰ë¥ ì€ INFO ë ˆë²¨ ì´ìƒë§Œ)
            console_handler = logging.StreamHandler()
            console_handler.setLevel(level)
            console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            console_handler.setFormatter(console_formatter)
            
            # ë£¨íŠ¸ ë¡œê±° ì„¤ì •
            root_logger = logging.getLogger()
            root_logger.setLevel(logging.DEBUG)  # ëª¨ë“  ë ˆë²¨ í—ˆìš©
            root_logger.addHandler(file_handler)
            root_logger.addHandler(console_handler)
            
        except Exception as e:
            print(f"ë¡œê¹… ì„¤ì • ì‹¤íŒ¨: {e}")
    
    @staticmethod
    def log_progress(message: str, level: int = logging.INFO, force_console: bool = False):
        """ì§„í–‰ë¥  ë¡œê¹… (ì½˜ì†” ì¶œë ¥ ì œì–´)"""
        # ë¡œê·¸ íŒŒì¼ì— í•­ìƒ ì €ì¥
        logging.log(level, f"{message}") # [ì§„í–‰ë¥ ]

    
    @staticmethod
    def generate_file_info(pandas_df: pd.DataFrame) -> str:
        """íŒŒì¼ ì •ë³´ ìš”ì•½ ìƒì„±"""
        try:
            file_size = sum(pandas_df['file_size'])
            
            file_info = f"""
            ë°ì´í„° ì²˜ë¦¬ ì •ë³´:
            --------------------------------------
            ì „ì²´ íŒŒì¼ ìˆ˜: {len(pandas_df)}
            ì „ì²´ íŒŒì¼ ìš©ëŸ‰: {DataProcessor.format_bytes(file_size)}
            --------------------------------------
            """
            return file_info
        except Exception as e:
            logging.error(f"íŒŒì¼ ì •ë³´ ìƒì„± ì‹¤íŒ¨: {e}")
            return "íŒŒì¼ ì •ë³´ ìƒì„± ì‹¤íŒ¨"
    
    @staticmethod
    def save_to_database(pandas_df: pd.DataFrame, db_path: str = './database/database.db') -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ì— ë°ì´í„° ì €ì¥"""
        import sqlite3
        
        try:
            os.makedirs(os.path.dirname(db_path), exist_ok=True)  # database ë””ë ‰í† ë¦¬ ìƒì„±
            conn = sqlite3.connect(db_path)
            
            # ë°ì´í„° ë² ì´ìŠ¤ì— ë°ì´í„° ì €ì¥
            pandas_df.to_sql('TB_meta_info', conn, if_exists='replace', index=False)
            
            conn.close()
            logging.info(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ: {db_path}")
            return db_path
        except Exception as e:
            logging.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise
    
    @staticmethod
    def append_to_database(pandas_df: pd.DataFrame, db_path: str = './database/database.db', 
                          table_name: str = 'TB_meta_info') -> str:
        """ë°ì´í„°ë² ì´ìŠ¤ì— ë°ì´í„° ì¶”ê°€ (ê¸°ì¡´ ë°ì´í„° ìœ ì§€)"""
        import sqlite3
        
        try:
            os.makedirs(os.path.dirname(db_path), exist_ok=True)  # database ë””ë ‰í† ë¦¬ ìƒì„±
            conn = sqlite3.connect(db_path)
            
            # ë°ì´í„°ë² ì´ìŠ¤ì— ë°ì´í„° ì¶”ê°€
            pandas_df.to_sql(table_name, conn, if_exists='append', index=False)
            
            conn.close()
            # ë¡œê·¸ ì œê±° - ë„ˆë¬´ ë§ì€ ë¡œê·¸ ì¶œë ¥ ë°©ì§€
            return db_path
        except Exception as e:
            logging.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            raise
    
    @staticmethod
    def validate_data_path(data_path: str) -> bool:
        """ë°ì´í„° ê²½ë¡œ ìœ íš¨ì„± ê²€ì‚¬"""
        path = Path(data_path)
        if not path.exists():
            logging.error(f"ë°ì´í„° ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {data_path}")
            return False
        if not path.is_dir():
            logging.error(f"ë°ì´í„° ê²½ë¡œê°€ ë””ë ‰í† ë¦¬ê°€ ì•„ë‹™ë‹ˆë‹¤: {data_path}")
            return False
        return True
    
    @staticmethod
    def process_data_pipeline(data_path: str, output_db_path: str = './database/database.db', 
                            max_workers: int = 4, batch_size: int = 1000) -> Tuple[pd.DataFrame, str, str]:
        """ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ (ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬)"""
        pipeline_start_time = time.time()
        
        try:
            # ë°ì´í„° ê²½ë¡œ ìœ íš¨ì„± ê²€ì‚¬
            if not DataProcessor.validate_data_path(data_path):
                raise ValueError(f"ìœ íš¨í•˜ì§€ ì•Šì€ ë°ì´í„° ê²½ë¡œ: {data_path}")
            
            logging.info(f"ë°ì´í„° ì²˜ë¦¬ ì‹œì‘: {data_path}")
    
            
            # 1. ì œë„ˆë ˆì´í„°ë¡œ íŒŒì¼ ê²½ë¡œ ìŠ¤íŠ¸ë¦¬ë°
            def file_path_stream():
                """íŒŒì¼ ê²½ë¡œë¥¼ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì œê³µ"""
                for path in Path(data_path).rglob('*'):
                    if path.is_file(): # and path.suffix.lower() != '.zip':
                        yield path
            
            # 2. ë°°ì¹˜ ì œë„ˆë ˆì´í„°
            def batch_generator(file_stream, batch_size):
                """íŒŒì¼ ìŠ¤íŠ¸ë¦¼ì„ ë°°ì¹˜ë¡œ ë¶„í• """
                batch = []
                for path in file_stream:
                    batch.append(path)
                    if len(batch) >= batch_size:
                        yield batch
                        batch = []  # ë°°ì¹˜ ì²˜ë¦¬ í›„ ì¦‰ì‹œ í•´ì œ
                if batch:  # ë§ˆì§€ë§‰ ë‚¨ì€ íŒŒì¼ë“¤
                    yield batch
            
            # ë°°ì¹˜ í¬ê¸° ê²°ì • (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³ ë ¤)
            if batch_size <= 0:
                batch_size = 1000  # ê¸°ë³¸ê°’
            

            # ì „ì²´ íŒŒì¼ ìˆ˜ ë° ë°°ì¹˜ ìˆ˜ ì¶”ì •
            estimated_total_files = 2745272  # ì‹¤ì œ íŒŒì¼ ìˆ˜
            estimated_total_batches = (estimated_total_files + batch_size - 1) // batch_size
            # logging.info(f"ì˜ˆìƒ ì´ ë°°ì¹˜ ìˆ˜: {estimated_total_batches}ê°œ, ì˜ˆìƒ ì´ íŒŒì¼ ìˆ˜: {estimated_total_files:,}ê°œ")
            
            # 3. ë°°ì¹˜ë³„ ì¦‰ì‹œ ì²˜ë¦¬ ë° í•´ì œ
            batch_count = 0
            total_files_processed = 0
            total_size_processed = 0
            last_logged_percentage_threshold = 0  # 10% ë‹¨ìœ„ ë¡œê¹…ì„ ìœ„í•œ ì„ê³„ê°’
            
            file_stream = file_path_stream()
            
            for batch_paths in batch_generator(file_stream, batch_size):
                # ë°°ì¹˜ ì²˜ë¦¬ (ì˜¬ë°”ë¥¸ ë°°ì¹˜ ì •ë³´ ì „ë‹¬)
                batch_df, batch_info = DataProcessor.process_batch(
                    batch_paths, batch_count + 1, estimated_total_batches, max_workers
                )
                
                # ì¦‰ì‹œ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
                if batch_count == 0:
                    DataProcessor.save_to_database(batch_df, output_db_path)
                else:
                    DataProcessor.append_to_database(batch_df, output_db_path)
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                batch_count += 1
                total_files_processed += len(batch_paths)
                total_size_processed += sum(batch_df['file_size'])
                
                # ë©”ëª¨ë¦¬ ì¦‰ì‹œ í•´ì œ
                del batch_df
                del batch_paths
                
                # ì§„í–‰ë¥  ê³„ì‚°
                if estimated_total_files > 0:
                    current_progress = (total_files_processed / estimated_total_files) * 100
                else:
                    current_progress = (batch_count / estimated_total_batches) * 100
                
                # 10% ë‹¨ìœ„ ë¡œê¹…
                if current_progress >= last_logged_percentage_threshold + 10 or (current_progress >= 99.9 and last_logged_percentage_threshold < 100):
                    DataProcessor.log_progress(
                        f"ì „ì²´ ì§„í–‰ë¥ : {current_progress:.1f}% ({total_files_processed:,}/{estimated_total_files:,} íŒŒì¼)", 
                        force_console=True
                    )
                    
                    # ë‹¤ìŒ ì„ê³„ê°’ ì—…ë°ì´íŠ¸
                    if current_progress >= 100:
                        last_logged_percentage_threshold = 100
                    else:
                        last_logged_percentage_threshold = int(current_progress) 
            
            # ì „ì²´ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            pipeline_end_time = time.time()
            total_elapsed = pipeline_end_time - pipeline_start_time
  
            # íŒŒì¼ ì •ë³´ ìƒì„±
            file_info = f"""
            ë°ì´í„° ì²˜ë¦¬ ì •ë³´:
            --------------------------------------
            ì „ì²´ íŒŒì¼ ìˆ˜: {total_files_processed:,}
            ì „ì²´ íŒŒì¼ ìš©ëŸ‰: {DataProcessor.format_bytes(total_size_processed)}
            --------------------------------------
            """
            
            logging.info(f"ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ (ì´ {total_elapsed:.1f}ì´ˆ)")
            logging.info(f"ì´ {total_files_processed:,}ê°œ íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ")
            
            # ë¹ˆ DataFrame ë°˜í™˜ (ë©”ëª¨ë¦¬ ì ˆì•½)
            return pd.DataFrame(), file_info, output_db_path
            
        except Exception as e:
            logging.error(f"ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {e}")
            raise